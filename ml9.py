# -*- coding: utf-8 -*-
"""ml9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JhAgQ0XfykapxB6oPWruU7EtM2pzOsfh
"""

from sklearn.datasets import make_blobs

# Generate synthetic data with 3 clusters and 200 samples
X, y = make_blobs(n_samples=200, centers=3, cluster_std=2.7)

# Print the shape of the data
print("Data shape:", X.shape)

# Print the first 5 samples
print("First 5 samples:\n", X[:5])

from sklearn.cluster import KMeans
import numpy as np

# Generate synthetic data with 3 clusters and 200 samples
X, y = make_blobs(n_samples=200, centers=3, cluster_std=2.7)

# Set the number of clusters and other KMeans parameters
n_clusters = 3
n_init = 10
max_iter = 300

# Perform KMeans clustering with random initialisation and 10 variations of initial means
kmeans = KMeans(n_clusters=n_clusters, init='random', n_init=n_init, max_iter=max_iter).fit(X)

# Print the lowest SSE value and final location of centroids
print("Lowest SSE value:", kmeans.inertia_)
print("Final location of centroids:\n", kmeans.cluster_centers_)

# Print the number of iterations to converge
print("Number of iterations to converge:", kmeans.n_iter_)

# Show the predicted labels for the first 10 points
print("Predicted labels for first 10 points:", kmeans.predict(X[:10]))

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Generate synthetic data with 3 clusters and 200 samples
X, y = make_blobs(n_samples=200, centers=3, cluster_std=2.7)

# Set the range of cluster values to try
cluster_range = range(2, 11)

# Perform KMeans clustering for each cluster value and calculate SSE
sse_values = []
for n_clusters in cluster_range:
    kmeans = KMeans(n_clusters=n_clusters, init='random', n_init=10, max_iter=300)
    kmeans.fit(X)
    sse_values.append(kmeans.inertia_)

# Plot SSE vs number of clusters to determine elbow point
plt.plot(cluster_range, sse_values, 'bx-')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.title('Elbow Method')
plt.show()

# Perform silhouette analysis to determine optimal number of clusters
silhouette_scores = []
for n_clusters in cluster_range:
    kmeans = KMeans(n_clusters=n_clusters, init='random', n_init=10, max_iter=300)
    kmeans.fit(X)
    silhouette_scores.append(silhouette_score(X, kmeans.labels_))

# Plot silhouette score vs number of clusters to determine optimal number of clusters
plt.plot(cluster_range, silhouette_scores, 'bx-')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.title('Silhouette Method')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# Load the Titanic dataset
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv'
titanic = pd.read_csv(url)

# Drop columns that won't be used for clustering
titanic.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)

# Fill in missing values for age with median age
titanic['Age'].fillna(titanic['Age'].median(), inplace=True)

# Encode categorical variables as numeric
label_encoder = LabelEncoder()
titanic['Sex'] = label_encoder.fit_transform(titanic['Sex'])
titanic['Embarked'] = label_encoder.fit_transform(titanic['Embarked'].astype(str))

# Standardize the numeric variables
scaler = StandardScaler()
titanic[['Age', 'Fare']] = scaler.fit_transform(titanic[['Age', 'Fare']])

# Perform KMeans clustering with KMeans++ initialization
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300)
kmeans.fit(titanic)

# Print the cluster centers
print(kmeans.cluster_centers_)

# Print the predicted labels for the first 10 passengers
print(kmeans.labels_[:10])